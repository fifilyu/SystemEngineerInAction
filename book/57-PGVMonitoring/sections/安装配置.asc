=== 安装配置

==== VictoriaMetrics

[source, bash]
----
docker pull victoriametrics/victoria-metrics:latest
# docker pull m.daocloud.io/docker.io/victoriametrics/victoria-metrics:latest


# 创建用户（不分配登录权限）
useradd -r -s /usr/sbin/nologin --home-dir /data3/victoria-metrics-data --create-home victoriametrics

# 数据保留6个月
docker run \
    -d \
    --restart always \
    --user `id -u victoriametrics`:`id -g victoriametrics` \
    --env LANG=en_US.UTF-8 \
    --env TZ=Asia/Shanghai \
    --restart=always \
    -p 8428:8428 \
    -v /data3/victoria-metrics-data:/var/lib/victoria-metrics-data \
    --name victoria_metrics \
    m.daocloud.io/docker.io/victoriametrics/victoria-metrics:latest \
    -retentionPeriod=180d \
    -selfScrapeInterval=10s \
    -storageDataPath=/var/lib/victoria-metrics-data


docker ps -a

docker logs victoria_metrics

ss -antpl|grep 8428
# LISTEN 0      1024         0.0.0.0:8428       0.0.0.0:*    users:(("docker-proxy",pid=433759,fd=4))

iptables -A INPUT -i br1 -p tcp -m tcp --dport 8428 -j DROP

iptables -I INPUT -s y.y.y.y/32 -i br1 -p tcp -m tcp --dport 8428 -j ACCEPT

pip3 install beautifulsoup4
curl -s http://localhost:8428/ | python3 -c "from bs4 import BeautifulSoup; import sys; soup=BeautifulSoup(sys.stdin.read(),'html.parser'); [print(text) for text in [t.strip() for t in soup.find_all(string=True) if t.strip()]]"

# 检查数据保留设置，单位：月
curl -s http://localhost:8428/metrics|grep retentionPeriod

# 写入测试数据
curl -d 'test_metric{label="value"} 123' http://localhost:8428/api/v1/import/prometheus

# 通过 PromQL 查询​
curl -s 'http://localhost:8428/api/v1/query' --data-urlencode 'query=test_metric' | jq
----

外部访问：
   http://x.x.x.x:8428/metrics

==== Grafana

[source, bash]
----

docker pull grafana/grafana-oss:latest
# docker pull docker.1ms.run/grafana/grafana-oss:latest

# 创建用户（不分配登录权限）
useradd -r -s /usr/sbin/nologin --home-dir /data3/grafana-data --create-home grafana

docker run \
    -d \
    --restart always \
    --user `id -u grafana`:`id -g grafana` \
    --env LANG=en_US.UTF-8 \
    --env TZ=Asia/Shanghai \
    --restart=always \
    -p 23000:3000 \
    -v /data3/grafana-data:/var/lib/grafana \
    --name grafana \
    docker.1ms.run/grafana/grafana-oss:latest

docker ps -a

docker logs grafana | head -n 20


ss -antpl|grep 23000
# LISTEN 0      1024         0.0.0.0:23000      0.0.0.0:*    users:(("docker-proxy",pid=514342,fd=4))

iptables -A INPUT -i br1 -p tcp -m tcp --dport 23000 -j DROP

iptables -I INPUT -s y.y.y.y/32 -i br1 -p tcp -m tcp --dport 23000 -j ACCEPT


http://x.x.x.x:23000/login
默认帐号和密码都是admin
----

.创建测试数据脚本
[source, bash]
----
cat << EOF > ~/victoria_metrics_push.py
import requests
import random
import time
from datetime import datetime
import sys

# 配置参数
PUSH_URL = "http://localhost:8428/api/v1/import/prometheus"
METRIC_NAME = "test_metric"
LABEL_NAME = "label"
LABEL_VALUE = "value"  # 固定标签值
DURATION = 60  # 运行时长(秒)
INTERVAL = 5   # 推送间隔(秒)

def generate_metric():
    """生成一个符合要求的指标数据点"""
    # 生成1到500的随机整数
    metric_value = random.randint(1, 500)
    # 获取当前时间戳（毫秒级）
    timestamp = int(time.time() * 1000)
    
    # 严格按照格式构建数据
    data = f'{METRIC_NAME}{{{LABEL_NAME}="{LABEL_VALUE}"}} {metric_value} {timestamp}'
    return data

def push_metric(data):
    """推送指标到VictoriaMetrics"""
    headers = {
        "Content-Type": "text/plain"
    }
    
    try:
        response = requests.post(PUSH_URL, data=data, headers=headers)
        response.raise_for_status()
        print(f"成功推送数据: {data}")
        return True
    except Exception as e:
        print(f"推送失败: {str(e)}")
        return False

def main():
    start_time = time.time()
    end_time = start_time + DURATION
    total_points = 0
    
    print(f"开始生成并推送指标数据到 {PUSH_URL}")
    print(f"将持续 {DURATION} 秒，每 {INTERVAL} 秒推送一次")
    print(f"开始时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-" * 50)
    
    while time.time() < end_time:
        # 生成指标数据
        metric_data = generate_metric()
        
        # 推送数据
        if push_metric(metric_data):
            total_points += 1
        
        # 计算剩余时间并等待
        remaining = max(0, end_time - time.time())
        sleep_time = min(INTERVAL, remaining)
        
        if sleep_time > 0:
            # 显示进度
            progress = min(100, int((1 - remaining/DURATION) * 100))
            print(f"进度: {progress}% | 已推送: {total_points} | 剩余时间: {int(remaining)}秒")
            time.sleep(sleep_time)
    
    print("-" * 50)
    print(f"数据生成和推送完成!")
    print(f"结束时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"总推送数据点: {total_points}")
    print(f"总运行时间: {time.time() - start_time:.2f}秒")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n程序被用户中断")
        sys.exit(0)
EOF
----

`python3 ~/victoria_metrics_push.py`

在Grafana中 `Connection` -> `Data Source` -> `Add new data source` -> `Prometheus` -> `Prometheus server URL` 设置为 `http://x.x.x.x:8428`

然后再 `http://x.x.x.x:23000/` 中的dashboards中查看提交的模拟数据

==== Prometheus和Node exporter

===== Node exporter

[source, bash]
----
mkdir -p ~/downloads/
wget https://github.com/prometheus/node_exporter/releases/download/v1.9.1/node_exporter-1.9.1.linux-amd64.tar.gz -O ~/downloads/node_exporter-1.9.1.linux-amd64.tar.gz
tar xf ~/downloads/node_exporter-1.9.1.linux-amd64.tar.gz -C /usr/local/
mv /usr/local/node_exporter-1.9.1.linux-amd64 /usr/local/node_exporter-1.9.1
mkdir -p /usr/local/node_exporter-1.9.1/bin
mv /usr/local/node_exporter-1.9.1/node_exporter /usr/local/node_exporter-1.9.1/bin/node_exporter
useradd -r -s /bin/false node_exporter
chown node_exporter:node_exporter /usr/local/node_exporter-1.9.1/bin/node_exporter
rm -f /usr/local/node_exporter-1.9.1/{LICENSE,NOTICE}
find /usr/local/node_exporter-1.9.1

mkdir -p /var/lib/node_exporter/textfile_collector
chown node_exporter:node_exporter /var/lib/node_exporter/textfile_collector

cat << EOF > /etc/systemd/system/node_exporter.service
[Unit]
Description=Node Exporter
Wants=network-online.target
After=network-online.target

[Service]
Type=simple
User=node_exporter
Group=node_exporter
ExecStart=/usr/local/node_exporter-1.9.1/bin/node_exporter \
    --web.listen-address=:29100 \
    --collector.systemd \
    --collector.systemd.unit-whitelist="(docker|sshd|nginx).service" \
    --collector.textfile.directory=/var/lib/node_exporter/textfile_collector

# 可选：限制内存等资源
MemoryMax=100M
Restart=on-failure
RestartSec=5s

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl enable node_exporter --now
systemctl start node_exporter

systemctl status node_exporter

ss -antpl|grep 29100
# LISTEN 0      1024               *:29100            *:*    users:(("node_exporter",pid=1426047,fd=3))

curl -Is http://localhost:29100/metrics

iptables -A INPUT -i br1 -p tcp -m tcp --dport 29100 -j DROP

iptables -I INPUT -s y.y.y.y/32 -i br1 -p tcp -m tcp --dport 29100 -j ACCEPT
----

外部访问：
   http://x.x.x.x:29100/metrics

===== Prometheus

[source, bash]
----
docker pull prom/prometheus:latest
# docker pull docker.1ms.run/prom/prometheus:latest

# 创建用户（不分配登录权限）
useradd -r -s /usr/sbin/nologin --home-dir /data3/prometheus-data --create-home prometheus
mkdir -p /data3/prometheus-data/{data,conf}
mkdir -p /data3/prometheus-data/conf/targets
chown -R prometheus:prometheus /data3/prometheus-data

cat << EOF > /data3/prometheus-data/conf/targets/node-exporters.json
[
  {
    "targets": ["x.x.x.x:29100"],
    "labels": {
      "env": "staging",
      "role": "host",
      "instance": "host-primary-01"
    }
  }
]
EOF

cat << EOF > /data3/prometheus-data/conf/prometheus.yml
global:
  scrape_interval: 15s # 全局抓取间隔
  evaluation_interval: 15s # 规则评估间隔
  # 外部标签，会在所有时间序列和告警中添加这些标签
  external_labels:
    cluster: 'my-cluster'
    replica: 'prometheus-01'

# 远程写入配置 - 这是指向VictoriaMetrics的关键！
remote_write:
  - url: "http://x.x.x.x:8428/api/v1/write"
    # 以下为可选但推荐的性能参数
    queue_config:
      max_samples_per_send: 10000 # 每个请求最多发送的样本数
      capacity: 20000 # 队列容量
      max_shards: 20 # 最大分片数，提高并发

# 抓取配置
scrape_configs:
  # 监控Prometheus自身
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # 监控Node Exporter（节点指标）
  - job_name: 'node-exporter-file-sd' # 任务名
    scrape_interval: 15s
    file_sd_configs: # 关键配置：文件服务发现
      - files:
          - '/etc/prometheus/targets/node-exporters.json' # 目标文件路径
        refresh_interval: 1m # 每1分钟重新加载一次文件
EOF

docker run \
    -d \
    --restart always \
    --user `id -u prometheus`:`id -g prometheus` \
    --env LANG=en_US.UTF-8 \
    --env TZ=Asia/Shanghai \
    --restart=always \
    -p 29090:9090 \
    -v /data3/prometheus-data/conf:/etc/prometheus \
    -v /data3/prometheus-data/data:/prometheus \
    --name prometheus \
    docker.1ms.run/prom/prometheus:latest \
    --config.file=/etc/prometheus/prometheus.yml \
    --storage.tsdb.path=/prometheus \
    --web.enable-lifecycle \
    --storage.tsdb.retention.time=180d

docker ps -a

docker logs prometheus | head -n 20


ss -antpl|grep 29090
# LISTEN 0      1024         0.0.0.0:29090      0.0.0.0:*    users:(("docker-proxy",pid=1434848,fd=4))

curl  http://localhost:29090/query

iptables -A INPUT -i br1 -p tcp -m tcp --dport 29090 -j DROP

iptables -I INPUT -s y.y.y.y/32 -i br1 -p tcp -m tcp --dport 29090 -j ACCEPT
----

外部访问：
  http://x.x.x.x:29090/query